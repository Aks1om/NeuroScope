from llama_cpp import Llama

MODEL_PATH = "C:/Users/artvo/.cache/huggingface/jan-nano-4b-Q8_0.gguf"  # укажи точное имя файла (или полный путь)

llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=4096,            # длина контекста (можно увеличить до 8192)
    n_threads=12,          # у тебя 6 ядер/12 потоков — указывай 12
    n_gpu_layers=28,       # 3070 Ti, 8 ГБ VRAM — оптимально 24–32 слоя, можно подобрать экспериментально
    verbose=True,
)

prompt = (
     """
<|im_start|>system
Ты — редактор автомобильного Telegram-канала. Всегда отвечай только готовым текстом поста для публикации, не добавляй размышления, пояснения, структуру или какие-либо шаги.
Пиши максимально по-человечески, живо, в стиле автожурнала, без лишней канцелярщины.
<|im_end|>
<|im_start|>user
Оформи новость как для Telegram-автоканала:
— Начни с броского, цепляющего утверждения или главного факта.
— Детали передавай короткими, ёмкими фразами с элементами автожаргона (например, “лошадей”, “робот”, “база”, “гибрид”).
— В конце добавь короткий вывод или личное мнение, чтобы пост был не сухим.
— Уложись в 3 абзаца.
— Не объясняй структуру и не включай размышления, выдай только финальный пост.

Новость:
Электромобиль Атом разработан под «поколенческое изменение отношения к автомобилю» — молодежь в крупных городах теперь предпочитает каршеринг и такси, но не спешит приобретать машину в личное пользование, об этом в интервью изданию РБК заявил гендиректор КАМАЗа Сергей Когогин. Напомним, КАМАЗ — ключевой инвестор проекта первого российского массового электромобиля.
Атом ориентирован под потребности каршеринга и такси по той причине, что в традиционных потребительских нишах «бороться с китайским автопромом невозможно», подчеркнул Когогин.
Ранее сообщалось о том, что на автозаводе «Москвич» завершается создание линий по серийному производству Атома. Выпуск новинки стартует в конце июля. Габаритная длина электромобиля — четыре метра, особенность конструкции — отсутствие центральных стоек кузова.

Пост:
<|im_end|>
<|im_start|>assistant

    """
)

output = llm(
    prompt,
    max_tokens=300,
    temperature=0.8,
    top_p=0.95
)

print(output["choices"][0]["text"].strip())